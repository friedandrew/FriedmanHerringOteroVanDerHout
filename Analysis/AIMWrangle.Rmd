---
title: "Wrangle"
author: "Jackie Van Der Hout"
date: "4/4/2022"
output: html_document
---
---
title: "Analyses"
author: "Jackie Van Der Hout"
date: "3/27/2022"
output: pdf_document
---
Prepare for work session: working directory and packages 
```{r}
knitr::opts_chunk$set(echo = FALSE)
getwd()
#load relevant packages once analyses is determined
library(tidyverse)
library(lubridate)
library(sf)
library(corrplot)
theme_set(theme_light())
```

Importing AIM Data
```{r}
AIMraw <- st_read("https://gis.blm.gov/arcgis/rest/services/hydrography/BLM_Natl_AIM_AquADat/MapServer/0/query?where=1%3D1&text=&objectIds=&time=&geometry=&geometryType=esriGeometryEnvelope&inSR=&spatialRel=esriSpatialRelIntersects&distance=&units=esriSRUnit_Foot&relationParam=&outFields=*&returnGeometry=true&returnTrueCurves=false&maxAllowableOffset=&geometryPrecision=&outSR=&havingClause=&returnIdsOnly=false&returnCountOnly=false&orderByFields=&groupByFieldsForStatistics=&outStatistics=&returnZ=false&returnM=false&gdbVersion=&historicMoment=&returnDistinctValues=false&resultOffset=&resultRecordCount=&returnExtentOnly=false&datumTransformation=&parameterValues=&rangeValues=&quantizationParameters=&featureEncoding=esriDefault&f=geojson&__ncforminfo=VmJvrSyZd1Eo2ftYuiZ2Csv3FXy8H7XoE-xtqGWFdpLoXXM_QrLBbJY-BtmB02LSjkYezZWjgMjkUQTDa0n0d7ugIiEzsdKzv1tB192rp3qdK8SWS-T_Vwxqlc6whlbENi3rrg4YqK40tpTjt0dJVWg3xwroBXxhGKT92BiR7Culk8jsk8g8cxPipo1GHHZ4oF9OZUqX8NX8gOXR3zsQRZxWmOIrkcEnN_JahqrcNjjDJXXh9ysVhdPK7UcQiEvMZy-n3fIQzqq-3YFqLGCq4ffzvu0ww4Pie6NuZD2CDkf9rilS-omAyiQq1LFJ0wVAl6PywwAseMQ4f6cDJXwLDn3BcTI3WP22jrsBYXm8SzrUZs5SGhJtMOOHN1-UMdNDFrO9rqMUXOQCTyKsmhFUFerLorYSAmMVohzr7cD03LvvdSkAQXKQpHQIcFuRqac69tN3eVlyV2IfsAA8yEcpU1GJKJx4jhwaVdXX9Sv4P1ACPp9mj3yqQ5zJqgAg0aFS6Z2O4GEd2B970yfKnbeicnbkgJTSCx0BQy3gtgkxCKujAYcWeMkvV5-dYcNF7lYQRaYvX8Eod5GUIbo8N8VNPF_USB9ZCQnlesBXLVMHXSOwH_bSq49wabgeZ2ZT7aZbJ1X8M_lXzzFieFrPzSKoOK_8_Bqcfde-Hdo8Jupndsn17KsTbpSH51tbBXOC6-3NJ0oBQuZAPkZ1I3pzzqb8sTFxIpYo2ia5lSHrZ5H-gxd753QRaQJBk1xWTyWgy8mON5hZYexZrjjUH4_C_SbY0Q%3D%3D")

#another data download option https://gbp-blm-egis.hub.arcgis.com/datasets/BLM-EGIS::blm-natl-aim-lotic-indicators-hub/about

#variable descriptions and metadata https://aim.landscapetoolbox.org/wp-content/uploads/2017/11/BLM-AIM-AquADat-Local-Feature-Class-Metadata.pdf

#more in depth metadata https://aim.landscapetoolbox.org/wp-content/uploads/2018/02/AIMLoticIndicatorMetadataUploadedDraft.pdf

#online metadata https://gis.blm.gov/arcgis/rest/services/hydrography/BLM_Natl_AIM_AquADat/MapServer/0
  
```

Exploratory Data Analysis 
```{r}
#repeat for both datasets
unique(AIMraw$PRJCT)

WAAIM <- AIMraw %>% 
  filter(PRJCT == "WA_SpokaneDO") 

SiteVisits <- WAAIM %>% 
  group_by(SITE_CD) %>% 
  summarise(columnname = n()) 
#all sites were only sampled once 

rm(AIMraw)

class()
colnames()
summary()
```

Data Tidying 
```{r}
#AIM Data

WAAIM_tidy <- WAAIM %>% 
  select(OBJECTID, SITE_CD, "StreamName" = STRM_NM, MIDLAT, MIDLONG, "StreamOrder" = STRM_ORDR, BTMLAT, BTMLONG, TRLAT, TRLONG, "ReachLength" = TOT_RCH_LEN, "PercentCover" = PCT_OVRHD_CVR, VEG_CMPLXTY, RPRN_VEG_CNPY_CVR, RPRN_VEG_UNDRSTRY_CVR, RPRN_VEG_GC, NON_NTVE_WDY, NTVE_WDY, NON_NTVE_HRB, NTVE_HRB, SDGE_RSH, OBSRVD_INVRT_RCHNSS, EXPCTD_INVRT_RCHNSS, MACROINVRTBRTE_CNT, "TotalN" = TTL_N, "PredictedN" = PRD_TTL_N, "TotalP" = TTL_P, "PredictedP" = PRD_TTL_P, "SpC" = SPCFC_CNDCTNCE, "PredictedSpC" = PRD_SPCFC_CNDCTNCE, pH, INSTNT_TEMP, "PercentPools" = PCT_PL, "AvgPoolDepth" = RES_PL_DEP, "NumberPools" = NM_PL, LWD_FREQ, "LWDVolume" = LWD_VL, "PercentFines2" = PCT_FN2, "PercentFines6" = PCT_FN6, D16, D84, D50, BNK_STBLTY, BNK_CVR_LWD, BNKFLL_HT, INCSN_HT, CHN_INCSN, THLWG_DEP_MN, PCT_DRY, BNKFLL_WT, WTTD_WT, "FloodWidth" = FLD_WT, "EntrenchmentRatio" = ENTRCH, SLPE, SNSTY, BVR_SGN, geometry)

rm(WAAIM)

#think about which lat-long columns it makes sense to use 

#run a correlation analyses within the dataset 
#remove highly correlated variables 

#set as sf object

#Dam Data
#set as sf object

#Background Map Data 
#set as sf object
```

Exploring the tidied data 
```{r}
#ggplot(USGS.flow.data, aes(x = discharge.mean)) +
  #geom_histogram(binwidth = 10) + 
  #scale_x_continuous(limits = c(0, 500))

#exploratory analysis of the means across states, regions, HUCs
#statistical tests, timeseries, etc 
#go through problem sets and see what might be applicable 
```

After data exploration: state research question, hypothesis and null hypothesis 
  -spatial - distance from some pollution source or dams changing water quality? 
  -any pattern across variables, time, spatial groupings, or mixture thereof 

Data Wrangling (see 03_DataExploration_Part2.rmd for more graph examples)
```{r}
#select for locations in WA 
#NTL.phys.data.PeterPaul <- filter(NTL.phys.data, lakename %in% c("Paul Lake", "Peter Lake")) 

#once wrangled, save data as csv
#write.csv(NTL.phys.data.PeterPaul1, row.names = FALSE, file = "../../Environmental_Data_Analytics_2022/Data/Processed/AIMProcessed.csv")
```

Calculating River Distance Options
```{r}
#river dist package https://github.com/mbtyers/riverdist

#alternate river distance calculation method https://fsdias.github.io/flow_distance.md/
#river mile calculator outside of R https://waterdata.usgs.gov/wa/nwis/current?type=rivermi

#bin sites into upstream or downstream of dam
#add column with mutate function correlating above or below dam
#another column for distance 
#to calculate that distance, it needs to be calculated along the stream 
#to calculate that use the riverdist package https://github.com/mbtyers/riverdist
#within that there is the river distance function
#I will need to grab the shapefiles (background map) 
#NHD+ tools will have the background map with the streams https://usgs-r.github.io/nhdplusTools/
#bin directly downstream and distance downstream 
#make sure that all of the files are in the same projection before doing the analysis #projection of the basemap is what the other two files should be in 
```

ndhplus & intersectr attempt
https://github.com/USGS-R/intersectr
```{r}
library(nhdplusTools)
site <- list(featureSource = #"nwissite",
               featureID = #"USGS-10128500")
line <- navigate_nldi(site, "UT", "")
site <- navigate_nldi(site, "UT", #"nwissite")

nhdp <- subset_nhdplus(#ut$nhdplus_comid,
                       #"nhdp_subset.gpkg",
                      #"download")
  
nhdplusTools::get_flowline_index(
  points = sites, 
  flines = flowline,
  searchradius = 100, #units of flowlines
  precision = 10), #max node spacing

#spatial intersection
#could be used to intersect dam and sample locations to flow lines? 
#install.packages("devtools")
library(devtools)
#be forewarned -- HUGE DOWNLOAD!!!!
#devtools::install_github("USGS-R/intersectr")
library(intersectr)
cells <- create_cell_geometry(x_coords, y_coords, nc_projections, catchment)
poly <- sf::st_as_sf(dplyr::select(catchment, ID = featureid))
weights <- calculate_area_intersection_weights(cells, poly)
dap_uri <- "#metadatalink"
execute_intersection(nc_file = dap_uri,
                     variable name = #"precipitation_amount", 
                intersection_weights = weights,
                cell_geometry = data_source_cells,
                x_var = x_coord, y_var = y_coord, t_var = t_coord, 
                start_datetime = #"",
                end_datetime = #"")

```


Visualize Data - check https://www.data-to-viz.com/ for more ideas 
```{r}

#ggplot(subset(PeterPaul.chem.nutrients, depth == 0),aes(x = daynum, y = temperature_C, color = as.factor(year4)))+
  #geom_point()+
  #facet_wrap(vars(lakename), nrow = 2)+
  #labs(y = "Temperature C") 

#nutrient plot, can be adjusted & done with facet_wrap
#Nutrientplot6 <-
 #ggplot(PeterPaul.chem.nutrients) +
  #geom_freqpoly(aes(x = tn_ug), color = "darkred") +
 #geom_freqpoly(aes(x = tp_ug), color = "darkblue") +
  #geom_freqpoly(aes(x = nh34), color = "blue") +
  #geom_freqpoly(aes(x = no23), color = "royalblue") +
  #geom_freqpoly(aes(x = po4), color = "red") 
#print(Nutrientplot6)
```

Space for any statistical analyses (t-test, ANOVAs, GLMs, linear regressions etc)
For example, comparing the means of a series of analytes that are and are not within X range proximity of an upstream dam.
See 06_Lab_GLMs.Rmd for outline and guides to selecting appropriate statistical tests. 

```{r correlation-check}
## Covariates - Cat is working on modifying this WDA code for our project

#If we wanted to explore some of the other variables that might help predict lake water quality, the function `corrplot` will allow us to create a correlation matrix whereby we can assess the correlations among various variables. We could then use this exploration to inform classification and/or covariate analysis with stressor-response modeling. 

# putting possible covariates that may affect the water quality together
LAGOScovariates <- LAGOSjoined %>%
  mutate(percentdeveloped = buffer500m_nlcd2011_pct_22 + buffer500m_nlcd2011_pct_23 + buffer500m_nlcd2011_pct_24,
         percentforest = buffer500m_nlcd2011_pct_41 + buffer500m_nlcd2011_ha_42 + buffer500m_nlcd2011_ha_43,
         percentrowcrops = buffer500m_nlcd2011_pct_82) %>%
  select(chla, tn, tp, doc, secchi,
         maxdepth,lake_area_ha, buffer500m_slope_mean,
         percentdeveloped, percentforest, percentrowcrops)

#need to keep as one dataset b/c cannot join sf datasets as we have others
WAAIM_tidy_numeric <- WAAIM_tidy %>% 
  select(OBJECTID, NON_NTVE_WDY, NTVE_WDY, NON_NTVE_HRB, NTVE_HRB, SDGE_RSH, OBSRVD_INVRT_RCHNSS, MACROINVRTBRTE_CNT, NumberPools, D16, D84, D50, BNK_STBLTY, BNK_CVR_LWD)

i <- c(2:15)  # Specify columns you want to change


WAAIM_tidy_numeric[ , i] <- apply(WAAIM_tidy_numeric[ , i], 2,            # Specify own function within apply
                    function(x) as.numeric(as.character(x)))

sapply(WAAIM_tidy_numeric, class)

merge(WAAIM_tidy, WAAIM_tidy_numeric, by='OBJECTID')


# create a correlation matrix. Necessitates only complete cases. Needs to be its own data matrix. Use cor to create the matrix so all variables are the columns and rows and their correlation btw each other is calculated. the dark blue diagonal down the middle is b/c when something correlated against itself is 1. Used to see how covariates are related aka correlated
WAAIMcorr <- cor(WAAIM_tidy)

?cor

# Intro Corrplot
# plot correlation matrix.
corrplot(LAGOScorr)

# customize so that only the upper half shows since the upper and lower (compared to the diagonal mirrors itself) with the type, diag = FALSE gets the self-correlation removed, endings change the text 
corrplot(LAGOScorr, type = "upper", diag = FALSE, tl.cex = 0.8, tl.col = "black")

#doc = dissolved organic carbon, more forest, higher slope correlated with more forest cover probs b/c harder to develop higher slope areas, bigger lakes correlated with deeper lakes. The size and color both change similarly for the correlation. 

#Once have correlation plot can run an AIC to decide what to actually include in your model. Can also do a lasso or other. These are parametric. Machine learning method can also be used to choose a subset. 
```


```{r}
##example only 
#checking for normality
shapiro.test(EPAair$Ozone[EPAair$Year == 2018])
shapiro.test(EPAair$Ozone[EPAair$Year == 2019])

#p-value less than 0.05 then reject null for 2018 and 2019 i.e. data do not follow normal distribution

#Compare variance using F-test (only)
var.test(EPAair$Ozone ~ EPAair$Year)

#p-value less than 0.05 then reject null for 2018 and 2019 i.e. true ratio not equal to one

ggplot(EPAair, aes(x = Ozone, color = as.factor(Year))) +
  geom_freqpoly()

# Format as a t-test
O3.twosample <- t.test(EPAair$Ozone ~ EPAair$Year)
O3.twosample
O3.twosample$p.value
#reject null hypothesis
#the means are not the same for 2018/2019 
#or there is not enough data for means to be the same 

# Format as a GLM
O3.twosample2 <- lm(EPAair$Ozone ~ EPAair$Year)
summary(O3.twosample2)
#GLM gives the same p-value

plot(O3.twosample2)

```

*Time series is probably not relevant to this project due to the nature of the data.* 

Spatial analyses
```{r}

```

**Notes for End of Project**
*Make sure that the README.md file is updated 
*If working on a specific basin, add image of basin in final report
*Check through grading checklist
*Go through project Rmd files provided by professors
*Go through crafting reports lessons for formatting tips 
*Consider making an RShiny dashboard if extra time / energy 